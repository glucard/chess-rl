{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chessrl.cnnextractor import CustomCNNExtractor\n",
    "import chess_gymnasium_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.envs import InvalidActionEnvDiscrete\n",
    "from sb3_contrib.common.maskable.evaluation import evaluate_policy\n",
    "from sb3_contrib.common.maskable.utils import get_action_masks\n",
    "# This is a drop-in replacement for EvalCallback\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from stable_baselines3.common.vec_env import VecNormalize, DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_mask_fn(env):\n",
    "    return env.unwrapped._get_action_mask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    def _init():\n",
    "        env = gym.make('chess_gymnasium_env/ChessEnv-v0')\n",
    "        env = ActionMasker(env, action_mask_fn)\n",
    "        return env\n",
    "    return _init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "\n",
    "# Number of parallel environments\n",
    "n_envs = 12\n",
    "\n",
    "# Create a vectorized environment\n",
    "env = DummyVecEnv([make_env() for _ in range(n_envs)])\n",
    "\n",
    "# OR for SubprocVecEnv (more efficient for computationally intensive tasks)\n",
    "# env = SubprocVecEnv([make_env() for _ in range(n_envs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomCNNExtractor,\n",
    "    features_extractor_kwargs=dict(features_dim=256)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('chess_gymnasium_env/ChessEnv-v0')#, render_mode=\"human\")\n",
    "# env = ActionMasker(env, action_mask_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for TensorBoard logs\n",
    "log_dir = './tb_logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MaskablePPO(\n",
    "    \"MlpPolicy\", \n",
    "    env,\n",
    "    policy_kwargs={\n",
    "        \"features_extractor_class\": CustomCNNExtractor,  # Use the custom CNN extractor\n",
    "        \"features_extractor_kwargs\": {\"features_dim\": 256},  # Output features dimension\n",
    "    },\n",
    "    gamma=0.995,              # Default gamma for PPO is 0.99, higher discount factor for long-term rewards\n",
    "    # learning_rate=0.00025,  # Typical learning rate for PPO (can be tuned)\n",
    "    # n_steps=2048,           # Number of steps to collect before updating the model (affects training stability)\n",
    "    # batch_size=64,          # Size of the batch used to update the model (affects the stability and speed of learning)\n",
    "    # n_epochs=10,            # Number of epochs to train on each batch (try increasing to fine-tune)\n",
    "    # gae_lambda=0.95,        # The lambda for Generalized Advantage Estimation (higher means more variance reduction)\n",
    "    # ent_coef=0.01,          # Coefficient for entropy regularization to encourage exploration\n",
    "    # clip_range=0.1,         # Lower value for more conservative updates (reduces large policy shifts)\n",
    "    # clip_range_vf=0.1,      # Clip range for the value function updates\n",
    "    # vf_coef=0.5,            # Coefficient for value function loss (fine-tune if necessary)\n",
    "    # max_grad_norm=0.5,      # Gradient clipping to prevent exploding gradients\n",
    "    # tensorboard_log=\"logs\", # TensorBoard logging directory (adjust to your needs)\n",
    "    seed=32,                # Random seed for reproducibility\n",
    "    verbose=1,              # Set verbosity level to 1 for progress logging\n",
    "    tensorboard_log=log_dir\n",
    ")\n",
    "print(f\"Model is running on device: {model.policy.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(5_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=20, reward_threshold=90, warn=False)\n",
    "\n",
    "# model.save(\"ppo_mask\")\n",
    "# del model # remove to demonstrate saving and loading\n",
    "\n",
    "# model = MaskablePPO.load(\"ppo_mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_env = gym.make('chess_gymnasium_env/ChessEnv-v0', render_mode=\"human\")\n",
    "render_env = ActionMasker(render_env, action_mask_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = render_env.reset()\n",
    "while True:\n",
    "    # Retrieve current action mask\n",
    "    action_masks = get_action_masks(render_env)\n",
    "    action, _states = model.predict(obs, action_masks=action_masks)\n",
    "    obs, reward, terminated, truncated, info = render_env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chess-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
